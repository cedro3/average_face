{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/cedro3/encoder4editing/blob/main/e4e_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ho2S8T_yJv1Q"
   },
   "source": [
    "# セットアップ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uuviq3qQkUFy"
   },
   "outputs": [],
   "source": [
    "# --- セットアップ ---\n",
    "\n",
    "import os\n",
    "os.chdir('/content')\n",
    "CODE_DIR = 'encoder4editing'\n",
    "\n",
    "!git clone https://github.com/cedro3/encoder4editing.git $CODE_DIR\n",
    "!wget https://github.com/ninja-build/ninja/releases/download/v1.8.2/ninja-linux.zip\n",
    "!sudo unzip ninja-linux.zip -d /usr/local/bin/\n",
    "!sudo update-alternatives --install /usr/bin/ninja ninja /usr/local/bin/ninja 1 --force\n",
    "os.chdir(f'./{CODE_DIR}')\n",
    "\n",
    "from argparse import Namespace\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "sys.path.append(\".\")\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from utils.common import tensor2im\n",
    "from models.psp import pSp  # we use the pSp framework to load the e4e encoder.\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# 学習済みパラメータのダウンロード\n",
    "import os\n",
    "import gdown\n",
    "os.makedirs('pretrained_models', exist_ok=True)\n",
    "gdown.download('https://drive.google.com/u/0/uc?id=1cUv_reLE6k3604or78EranS7XzuVMWeO', 'pretrained_models/e4e_ffhq_encode.pt', quiet=False)\n",
    "\n",
    "# ランドマークデータのダウンロード\n",
    "! wget http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n",
    "! bzip2 -dk shape_predictor_68_face_landmarks.dat.bz2\n",
    "\n",
    "# モデルに学習済みパラメータをロード\n",
    "model_path = 'pretrained_models/e4e_ffhq_encode.pt'  ####\n",
    "ckpt = torch.load(model_path, map_location='cpu')\n",
    "opts = ckpt['opts']\n",
    "opts['checkpoint_path'] = model_path\n",
    "opts= Namespace(**opts)\n",
    "net = pSp(opts)\n",
    "net.eval()\n",
    "net.cuda()\n",
    "print('Model successfully loaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o6oqf8JwzK0K"
   },
   "source": [
    "# 事前準備\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Ld45KbIF45C"
   },
   "outputs": [],
   "source": [
    "# --- 顔画像の切り出し ---\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "if os.path.isdir('align'):\n",
    "     shutil.rmtree('align')\n",
    "os.makedirs('align', exist_ok=True)\n",
    "\n",
    "def run_alignment(image_path):\n",
    "  import dlib\n",
    "  from utils.alignment import align_face\n",
    "  predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "  aligned_image = align_face(filepath=image_path, predictor=predictor) \n",
    "  return aligned_image \n",
    "\n",
    "path = './images'\n",
    "files = sorted(os.listdir(path))\n",
    "for i, file in enumerate(tqdm(files)):\n",
    "  if file=='.ipynb_checkpoints':\n",
    "     continue\n",
    "  input_image = run_alignment(path+'/'+file)\n",
    "  input_image.resize((256,256))\n",
    "  input_image.save('./align/'+file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rxbCdavcfXNd"
   },
   "outputs": [],
   "source": [
    "# --- 潜在変数の推定 ---\n",
    "\n",
    "if os.path.isdir('vec_pic'):\n",
    "     shutil.rmtree('vec_pic')\n",
    "os.makedirs('vec_pic', exist_ok=True)\n",
    "\n",
    "if os.path.isdir('vec'):\n",
    "     shutil.rmtree('vec')\n",
    "os.makedirs('vec', exist_ok=True)\n",
    "\n",
    "img_transforms = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
    "\n",
    "path = './align'\n",
    "files = sorted(os.listdir(path))\n",
    "for i, file in enumerate(tqdm(files)):\n",
    "  if file=='.ipynb_checkpoints':\n",
    "     continue\n",
    "  input_image = Image.open(path+'/'+file)\n",
    "  transformed_image = img_transforms(input_image)\n",
    "  with torch.no_grad():\n",
    "     images, latents = net(transformed_image.unsqueeze(0).to('cuda').float(), randomize_noise=False, return_latents=True)\n",
    "     result_image, latent = images[0], latents[0]\n",
    "     tensor2im(result_image).save('./vec_pic/'+file) # vec_pic 保存\n",
    "     torch.save(latents, './vec/'+file[:-4]+'.pt') # vec  保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HvA3EJXX3ZxH"
   },
   "outputs": [],
   "source": [
    "# --- 元画像と生成画像の表示 ---\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "def display_pic(folder):\n",
    "    fig = plt.figure(figsize=(30, 40))\n",
    "    files = os.listdir(folder)\n",
    "    files.sort()\n",
    "    for i, file in enumerate(files):\n",
    "        img = Image.open(folder+'/'+file)    \n",
    "        images = np.asarray(img)\n",
    "        ax = fig.add_subplot(10, 10, i+1, xticks=[], yticks=[])\n",
    "        image_plt = np.array(images)\n",
    "        ax.imshow(image_plt)\n",
    "        ax.set_xlabel(folder+'/'+file, fontsize=15)               \n",
    "    plt.show()\n",
    "    plt.close()  \n",
    "\n",
    "display_pic('align')\n",
    "display_pic('vec_pic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n-kszSzaLAkw"
   },
   "source": [
    "# 画像編集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fH1ewDunUEo6"
   },
   "outputs": [],
   "source": [
    "#@title 設定\n",
    "latent = \"03.pt\"#@param {type:\"string\"}\n",
    "direction = \"age\" #@param [\"age\", \"pose\", \"smile\", \"age+pose\"] {allow-input: true}\n",
    "min = -50 #@param {type:\"slider\", min:-50, max:0, step:10}\n",
    "max = 50 #@param {type:\"slider\", min:0, max:50, step:10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cliYeQUdK8rr"
   },
   "outputs": [],
   "source": [
    "# --- 静止画の生成 ---\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "if os.path.isdir('pic'):\n",
    "     shutil.rmtree('pic')\n",
    "os.makedirs('pic', exist_ok=True)\n",
    "\n",
    "from editings import latent_editor\n",
    "from tqdm import trange\n",
    "\n",
    "folder = 'vec'\n",
    "latents = torch.load(folder+'/'+latent)\n",
    "editor = latent_editor.LatentEditor(net.decoder, False)\n",
    "\n",
    "interfacegan_directions = {\n",
    "        'age': 'editings/interfacegan_directions/age.pt',\n",
    "        'smile': 'editings/interfacegan_directions/smile.pt',\n",
    "        'pose': 'editings/interfacegan_directions/pose.pt',\n",
    "        'age+pose':  'editings/interfacegan_directions/age+pose.pt'\n",
    "    }\n",
    "\n",
    "interfacegan_direction = torch.load(interfacegan_directions[direction]).cuda()\n",
    "cnt = 0\n",
    "\n",
    "for i in trange(0, min, -1, desc='0 -> min'):\n",
    "     result = editor.apply_interfacegan(latents, interfacegan_direction, factor=i).resize((512,512))\n",
    "     result.save('./pic/'+str(cnt).zfill(6)+'.jpg')\n",
    "     cnt +=1\n",
    "\n",
    "for i in trange(min, max, desc='min -> max'):\n",
    "     result = editor.apply_interfacegan(latents, interfacegan_direction, factor=i).resize((512,512))\n",
    "     result.save('./pic/'+str(cnt).zfill(6)+'.jpg')\n",
    "     cnt +=1\n",
    "\n",
    "for i in trange(max, 0, -1, desc='max -> 0'):\n",
    "     result = editor.apply_interfacegan(latents, interfacegan_direction, factor=i).resize((512,512))\n",
    "     result.save('./pic/'+str(cnt).zfill(6)+'.jpg')\n",
    "     cnt +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "czgM1jHqK82m"
   },
   "outputs": [],
   "source": [
    "# --- mp4動画の作成 ---\n",
    "\n",
    "# 既に output.mp4 があれば削除する\n",
    "import os\n",
    "if os.path.exists('./output.mp4'):\n",
    "   os.remove('./output.mp4')\n",
    "\n",
    "# pic フォルダーの静止画から動画を作成\n",
    "! ffmpeg -r 30 -i pic/%6d.jpg\\\n",
    "               -vcodec libx264 -pix_fmt yuv420p output.mp4\n",
    "\n",
    "# movieフォルダへ名前を付けてコピー\n",
    "import shutil\n",
    "os.makedirs('movie', exist_ok=True)\n",
    "shutil.copy('output.mp4', 'movie/'+direction+'_'+latent[:-3]+'.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a8Rhw4QBK_cw"
   },
   "outputs": [],
   "source": [
    "# --- mp4動画の再生 ---\n",
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "\n",
    "mp4 = open('./output.mp4', 'rb').read()\n",
    "data_url = 'data:video/mp4;base64,' + b64encode(mp4).decode()\n",
    "HTML(f\"\"\"\n",
    "<video width=\"50%\" height=\"50%\" controls>\n",
    "      <source src=\"{data_url}\" type=\"video/mp4\">\n",
    "</video>\"\"\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "e4e_demo",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
